# transformer_solver/trainer.py

import torch
import torch.nn.functional as F
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
from torch.utils.data import DataLoader

from tqdm import tqdm
import os
import time
from datetime import datetime
import logging
from collections import defaultdict
import json

# --- í•µì‹¬ ëª¨ë“ˆ ì„í¬íŠ¸ ---
from .model import PocatModel, PrecomputedCache, reshape_by_heads
from .solver_env import PocatEnv, BATTERY_NODE_IDX
from .expert_dataset import ExpertReplayDataset, expert_collate_fn
from .utils.common import TimeEstimator, clip_grad_norms, unbatchify, batchify

# --- ì‹œê°í™” ëª¨ë“ˆ ì„í¬íŠ¸ ---
from graphviz import Digraph
from common.data_classes import LDO, BuckConverter # (common)
from .definitions import FEATURE_INDEX, NODE_TYPE_LOAD, NODE_TYPE_IC


def update_progress(pbar, metrics, step):
    """ tqdm ì§„í–‰ë¥  í‘œì‹œì¤„ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. """
    if pbar is None:
        return
    
    metrics_str = (
        f"Loss: {metrics['Loss']:.4f} "
        f"($Avg: {metrics['Avg Cost']:.2f}, $Min: {metrics['Min Cost']:.2f})"
    )
    pbar.set_postfix_str(metrics_str, refresh=False)
    pbar.update(1)


def cal_model_size(model, log_func):
    """ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ë° ë²„í¼ í¬ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤. """
    param_count = sum(p.nelement() for p in model.parameters() if p.requires_grad)
    buffer_count = sum(b.nelement() for b in model.buffers())
    log_func(f'ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {param_count:,}')
    log_func(f'ëª¨ë¸ ë²„í¼ ìˆ˜: {buffer_count:,}')

class PocatTrainer:
    """
    PocatModelê³¼ PocatEnvë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨, ê²€ì¦, í…ŒìŠ¤íŠ¸ë¥¼
    ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. (A2C ê¸°ë°˜)
    """
    
    def __init__(self, args, env: PocatEnv, device: str):
        self.args = args
        self.env = env
        self.is_ddp = args.ddp
        self.local_rank = args.local_rank
        self.device = device

        self.result_dir = args.result_dir
        self.log = args.log

        # --- 1. ëª¨ë¸ ì´ˆê¸°í™” ë° DDP ë˜í•‘ ---
        self.model = PocatModel(**args.model_params).to(self.device)
        
        if self.is_ddp:
            self.model = DDP(
                self.model, 
                device_ids=[self.local_rank], 
                find_unused_parameters=False # (ëª¨ë¸ì€ ëª¨ë“  íŒŒë¼ë¯¸í„° ì‚¬ìš©)
            )
        
        if self.local_rank <= 0:
            cal_model_size(self.model, self.log)
        
        # --- 2. ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ---
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=float(args.optimizer_params['optimizer']['lr']),
            weight_decay=float(args.optimizer_params['optimizer'].get('weight_decay', 0)),
        )
        
        if args.optimizer_params['scheduler']['name'] == 'MultiStepLR':
            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
                self.optimizer,
                milestones=args.optimizer_params['scheduler']['milestones'],
                gamma=args.optimizer_params['scheduler']['gamma']
            )
        else:
            raise NotImplementedError
            
        self.start_epoch = 1

        # --- 3. ëª¨ë¸ ë¡œë“œ (Checkpoint) ---
        if args.load_path is not None:
            self.log(f"ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘: {args.load_path}")
            try:
                checkpoint = torch.load(args.load_path, map_location=device)
                
                # DDP/ì¼ë°˜ ëª¨ë¸ ìƒíƒœ í˜¸í™˜ ë¡œë“œ
                model_to_load = self.model.module if self.is_ddp else self.model
                model_to_load.load_state_dict(checkpoint['model_state_dict'])
                
                if not args.test_only: # í›ˆë ¨ ì¬ê°œ ì‹œ
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    self.start_epoch = checkpoint['epoch'] + 1
                self.log("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
            except Exception as e:
                self.log(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. ë¬´ì‘ìœ„ ì´ˆê¸°í™”ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

        self.time_estimator = TimeEstimator(log_fn=self.log)

        # --- 4. ê²€ì¦(Evaluate)ìš© ë°ì´í„°ì…‹ ---
        self.eval_batch_size = getattr(args, "eval_batch_size", 128)
        if self.local_rank <= 0: # 0ë²ˆ GPUì—ì„œë§Œ ìƒì„±
            with torch.no_grad():
                self._eval_td_fixed = self.env.generator(
                    batch_size=self.eval_batch_size
                ).to(self.device)
        self.best_eval_bom = float("inf")

    def pretrain_critic(self, expert_data_path: str, pretrain_epochs: int = 5):
        """
        'ì •ë‹µì§€(Expert)' ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ A2C ëª¨ë¸ì˜ Critic(Value Head)ë§Œ
        ì§€ë„í•™ìŠµ ë°©ì‹ìœ¼ë¡œ ì‚¬ì „í›ˆë ¨í•©ë‹ˆë‹¤.
        """
        args = self.args
        self.log("=================================================================")
        self.log(f"ğŸ§  Critic ì‚¬ì „í›ˆë ¨(Pre-training) ì‹œì‘...")
        
        try:
            expert_dataset = ExpertReplayDataset(
                expert_data_path=expert_data_path, 
                env=self.env, 
                device=self.device
            )
            if len(expert_dataset) == 0:
                self.log("âŒ ì˜¤ë¥˜: 'ì •ë‹µì§€' ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆì–´ ì‚¬ì „í›ˆë ¨ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            expert_loader = DataLoader(
                expert_dataset,
                batch_size=args.batch_size, # í›ˆë ¨ ë°°ì¹˜ í¬ê¸° ì¬ì‚¬ìš©
                shuffle=True,
                num_workers=0,
                collate_fn=expert_collate_fn # TensorDictìš© ì»¤ìŠ¤í…€ Collate
            )
        except Exception as e:
            self.log(f"âŒ ì˜¤ë¥˜: 'ì •ë‹µì§€' ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return

        # Critic íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµí•˜ëŠ” ë³„ë„ì˜ ì˜µí‹°ë§ˆì´ì € ìƒì„±
        model_to_train = self.model.module if self.is_ddp else self.model
        critic_params = list(model_to_train.decoder.value_head.parameters()) + \
                        list(model_to_train.decoder.Wq_context.parameters()) + \
                        list(model_to_train.decoder.multi_head_combine.parameters())
                        
        critic_optimizer = torch.optim.AdamW(
            critic_params,
            lr=float(args.optimizer_params['optimizer']['lr'])
        )

        self.model.train()

        for epoch in range(1, pretrain_epochs + 1):
            pbar = tqdm(expert_loader, desc=f"Critic Pre-train Epoch {epoch}/{pretrain_epochs}", dynamic_ncols=True)
            total_v_loss = 0
            
            for state_td_batch, target_reward_batch in pbar:
                critic_optimizer.zero_grad()
                
                # (B, 1, ...) -> (B, ...)
                state_td_batch = state_td_batch.squeeze(1)
                
                # --- ëª¨ë¸ ì¸ì½”ë”© ë° ìºì‹œ ìƒì„± ---
                prompt_embedding = model_to_train.prompt_net(
                    state_td_batch["scalar_prompt_features"], 
                    state_td_batch["matrix_prompt_features"]
                )
                encoded_nodes = model_to_train.encoder(state_td_batch, prompt_embedding)
                
                glimpse_key = reshape_by_heads(model_to_train.decoder.Wk_glimpse(encoded_nodes), model_to_train.decoder.head_num)
                glimpse_val = reshape_by_heads(model_to_train.decoder.Wv_glimpse(encoded_nodes), model_to_train.decoder.head_num)
                logit_key_connect = model_to_train.decoder.Wk_connect_logit(encoded_nodes).transpose(1, 2)
                logit_key_spawn = model_to_train.decoder.Wk_spawn_logit(encoded_nodes).transpose(1, 2)
                
                cache = PrecomputedCache(
                    encoded_nodes, glimpse_key, glimpse_val, 
                    logit_key_connect, logit_key_spawn
                )
                
                # --- ë””ì½”ë” í˜¸ì¶œ (Valueë§Œ ì‚¬ìš©) ---
                _, _, _, predicted_value = model_to_train.decoder(state_td_batch, cache)
                
                # V_Loss ê³„ì‚°: Criticì˜ ì˜ˆì¸¡ vs "ì •ë‹µì§€"ì˜ ì‹¤ì œ ë³´ìƒ
                critic_loss = F.mse_loss(predicted_value, target_reward_batch)
                
                critic_loss.backward()
                critic_optimizer.step()
                
                total_v_loss += critic_loss.item()
                pbar.set_postfix({"V_Loss (Pre)": f"{critic_loss.item():.4f}"})

            self.log(f"Critic Pre-train Epoch {epoch} | Avg V_Loss: {total_v_loss / len(expert_loader):.4f}")

        self.log("âœ… Critic ì‚¬ì „í›ˆë ¨ ì™„ë£Œ.")
        self.log("=================================================================")

    def run(self):
        """ ë©”ì¸ í›ˆë ¨ ë£¨í”„ (A2C) """
        args = self.args
        self.time_estimator.reset(self.start_epoch)
        
        if args.test_only:
            self.test()
            return

        for epoch in range(self.start_epoch, args.trainer_params['epochs'] + 1):
            if self.local_rank <= 0:
                self.log('=' * 60)
            
            self.model.train()
            
            # (DDP) DDP Samplerê°€ ì—í­ë§ˆë‹¤ ì‹œë“œë¥¼ ë³€ê²½í•˜ë„ë¡ ì„¤ì •
            if self.is_ddp and hasattr(self.env_dataset, 'sampler'):
                self.env_dataset.sampler.set_epoch(epoch)
            
            total_steps = args.trainer_params['train_step']
            
            # (DDP) 0ë²ˆ GPUì—ì„œë§Œ tqdm ì§„í–‰ë¥  í‘œì‹œ
            train_pbar = None
            if self.local_rank <= 0:
                train_pbar = tqdm(
                    total=total_steps,
                    desc=f"Epoch {epoch}",
                    dynamic_ncols=True,
                )
            
            total_loss = 0.0
            total_cost = 0.0
            total_policy_loss = 0.0
            total_critic_loss = 0.0
            min_epoch_cost = float('inf')

            for step in range(1, total_steps + 1):
                self.optimizer.zero_grad()
                
                # 1. í™˜ê²½ ë¦¬ì…‹
                # (DDP ì‚¬ìš© ì‹œ, ê° GPUëŠ” B/N ê°œì˜ ë°°ì¹˜ë¥¼ ì²˜ë¦¬)
                td = self.env.reset(batch_size=args.batch_size)
                
                # 2. POMO (Multi-Start) í™•ì¥
                num_starts = self.env.generator.num_loads
                if args.num_pomo_samples > 1:
                    # (Load ê°œìˆ˜(num_starts)ë§Œí¼ë§Œ í™•ì¥)
                    td = batchify(td, num_starts)
                
                # 3. ëª¨ë¸ í¬ì›Œë“œ (ì†”ë£¨ì…˜ ìƒì„±)
                out = self.model(
                    td, self.env, decode_type='sampling', pbar=train_pbar,
                    status_msg=f"Epoch {epoch}", log_fn=self.log,
                    log_idx=args.log_idx, log_mode=args.log_mode
                )
                
                # 4. A2C ì†ì‹¤ ê³„ì‚°
                # (B, N_pomo)
                reward = out["reward"].view(-1, num_starts)
                log_likelihood = out["log_likelihood"].view(-1, num_starts)
                value = out["value"].view(-1, num_starts)

                # Critic Loss (V(s)ê°€ ì‹¤ì œ ë³´ìƒ(G)ì„ ì˜ˆì¸¡í•˜ë„ë¡)
                critic_loss = F.mse_loss(value, reward)

                # Policy Loss (Actor)
                advantage = reward - value.detach() # Baseline = V(s)
                policy_loss = -(advantage * log_likelihood).mean()

                # Total Loss (A2C)
                loss = policy_loss + 0.5 * critic_loss
                
                # 5. ì—­ì „íŒŒ ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
                loss.backward()
                
                max_norm = float(self.args.optimizer_params.get('max_grad_norm', 0))
                if max_norm > 0:
                    clip_grad_norms(self.optimizer.param_groups, max_norm=max_norm)
                self.optimizer.step()

                # (DDP) ëª¨ë“  GPUì˜ í†µê³„ë¥¼ ì§‘ê³„
                if self.is_ddp:
                    dist.all_reduce(loss, op=dist.ReduceOp.AVG)
                    dist.all_reduce(policy_loss, op=dist.ReduceOp.AVG)
                    dist.all_reduce(critic_loss, op=dist.ReduceOp.AVG)
                    # (min_costëŠ” all_reduce(op=dist.ReduceOp.MIN) í•„ìš”)
                
                # (DDP) 0ë²ˆ GPUì—ì„œë§Œ ë¡œê·¸ ê¸°ë¡
                if self.local_rank <= 0:
                    avg_cost = -reward.mean().item()
                    min_batch_cost = -reward.max().item()
                    min_epoch_cost = min(min_epoch_cost, min_batch_cost)

                    total_loss += loss.item()
                    total_cost += avg_cost
                    total_policy_loss += policy_loss.item()
                    total_critic_loss += critic_loss.item()

                    update_progress(
                        train_pbar,
                        {
                            "Loss": loss.item(),
                            "Avg Cost": total_cost / step,
                            "Min Cost": min_epoch_cost,
                        },
                        step
                    )

            if train_pbar:
                train_pbar.close()

            # (DDP) 0ë²ˆ GPUì—ì„œë§Œ ì—í­ ìš”ì•½, ê²€ì¦, ì €ì¥
            if self.local_rank <= 0:
                epoch_summary = (
                    f"Epoch {epoch}/{args.trainer_params['epochs']} | "
                    f"Avg Loss {total_loss / total_steps:.4f} | "
                    f"P_Loss {total_policy_loss / total_steps:.4f} | "
                    f"V_Loss {total_critic_loss / total_steps:.4f} | "
                    f"Min Cost ${min_epoch_cost:.2f}"
                )
                tqdm.write(epoch_summary)
                self.log(epoch_summary)
                
                # --- ê²€ì¦ (Evaluate) ---
                val = self.evaluate(epoch)
                self.log(f"[Eval] Epoch {epoch} | Avg BOM ${val['avg_bom']:.2f} | Min BOM ${val['min_bom']:.2f}")

                # --- ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ---
                if (epoch % args.trainer_params['model_save_interval'] == 0) or \
                   (epoch == args.trainer_params['epochs']):
                       
                    save_path = os.path.join(args.result_dir, f'epoch-{epoch}.pth')
                    self.log(f"ëª¨ë¸ ì €ì¥ ì¤‘... (Epoch {epoch} -> {save_path})")
                    self._run_test_visualization(epoch, is_best=False) # ì‹œê°í™”
                    
                    model_state_dict = self.model.module.state_dict() if self.is_ddp else self.model.state_dict()
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model_state_dict,
                        'optimizer_state_dict': self.optimizer.state_dict(),
                    }, save_path)

            self.scheduler.step()

            if self.local_rank <= 0:
                self.time_estimator.print_est_time(epoch, args.trainer_params['epochs'])
            
            if self.is_ddp:
                dist.barrier() # ì—í­ ì¢…ë£Œ ì‹œ ëª¨ë“  GPU ë™ê¸°í™”

        if self.local_rank <= 0:
            self.log(" *** í›ˆë ¨ ì™„ë£Œ *** ")


    @torch.no_grad()
    def evaluate(self, epoch: int):
        """ ê³ ì •ëœ ê²€ì¦ ì…‹(Validation Set)ì— ëŒ€í•´ Greedy íƒìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. """
        self.model.eval()
        
        # (ê³ ì •ëœ ê²€ì¦ ë°ì´í„°ì…‹ ì‚¬ìš©)
        td_eval = self.env._reset(self._eval_td_fixed.clone())
        
        # POMO (Load ê°œìˆ˜ë§Œí¼) í™•ì¥
        eval_samples, start_nodes_idx = self.env.select_start_nodes(td_eval)
        if eval_samples > 1:
            td_eval = batchify(td_eval, eval_samples)
            # (pom_start ë¡œì§ì€ env._resetì—ì„œ ì²˜ë¦¬ë˜ì—ˆë‹¤ê³  ê°€ì •)

        out = self.model(
            td_eval, self.env, decode_type='greedy',
            pbar=None, status_msg="Eval",
            log_fn=self.log, log_idx=self.args.log_idx, log_mode='progress'
        )

        # (B, N_pomo)
        reward = out["reward"].view(self.eval_batch_size, eval_samples)
        # ì¸ìŠ¤í„´ìŠ¤ë³„ ìµœê³  ì ìˆ˜ (B,)
        best_reward_per_instance = reward.max(dim=1)[0]

        avg_bom = -best_reward_per_instance.mean().item()
        min_bom = -best_reward_per_instance.max().item()

        # CSV ë¡œê·¸
        csv_path = os.path.join(self.result_dir, "val_log.csv")
        header = not os.path.exists(csv_path)
        with open(csv_path, "a", encoding="utf-8") as f:
            if header: f.write("epoch,avg_bom,min_bom,decode_type\n")
            f.write(f"{epoch},{avg_bom:.4f},{min_bom:.4f},greedy\n")

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if avg_bom < self.best_eval_bom:
            self.best_eval_bom = avg_bom
            save_path = os.path.join(self.result_dir, "best_cost.pth")
            self.log(f"[Eval] âœ… ìƒˆ ìµœê³  ì„±ëŠ¥ ë‹¬ì„± ${avg_bom:.2f} (min=${min_bom:.2f}) -> {save_path} ì €ì¥")
            
            # í…ŒìŠ¤íŠ¸ ì‹œê°í™” ì‹¤í–‰
            self._run_test_visualization(epoch, is_best=True)
            
            model_state_dict = self.model.module.state_dict() if self.is_ddp else self.model.state_dict()
            torch.save({
                'epoch': epoch,
                'model_state_dict': model_state_dict,
                'optimizer_state_dict': self.optimizer.state_dict(),
            }, save_path)

        return {"avg_bom": avg_bom, "min_bom": min_bom}

    def test(self):
        """ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì¶”ë¡ )ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. """
        self.model.eval()
        self.log("=" * 60)
        self.log("ğŸ”¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì¶”ë¡ ) ì‹œì‘...")
        self._run_test_visualization(epoch=0, is_best=False)
        self.log("=" * 60)

    @torch.no_grad()
    def _run_test_visualization(self, epoch: int, is_best: bool = False):
        """
        ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•´ ì¶”ë¡ ì„ ì‹¤í–‰í•˜ê³ ,
        ìµœì¢… í…ì„œ(TensorDict) ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒŒì›ŒíŠ¸ë¦¬ ì‹œê°í™”(PNG)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        """
        self.model.eval()
        args = self.args

        if is_best:
            log_prefix = f"[Test Viz @ Epoch {epoch} (BEST)]"
            filename_prefix = f"epoch_{epoch}_best"
        elif epoch > 0:
            log_prefix = f"[Test Viz @ Epoch {epoch}]"
            filename_prefix = f"epoch_{epoch}"
        else:
            log_prefix = "[Test Viz (Standalone)]"
            filename_prefix = "test_solution"

        self.log(f"{log_prefix} ì¶”ë¡  ë° ì‹œê°í™” ì‹œì‘...")

        # 1. ë‹¨ì¼ ë°°ì¹˜(B=1)ë¡œ í™˜ê²½ ë¦¬ì…‹
        td = self.env.reset(batch_size=1)
        
        # 2. POMO í™•ì¥
        test_samples, start_nodes_idx = self.env.select_start_nodes(td)
        if args.test_num_pomo_samples > test_samples:
             self.log(f"Warning: test_num_pomo_samples({args.test_num_pomo_samples})ê°€ Load ê°œìˆ˜({test_samples})ë³´ë‹¤ í½ë‹ˆë‹¤.")
        
        # (Load ê°œìˆ˜ë§Œí¼ë§Œ í™•ì¥)
        td = batchify(td, test_samples)
        
        # (POMO ì‹œì‘ ìƒíƒœ ì ìš© - solver_envê°€ _resetì—ì„œ ì²˜ë¦¬)
        
        pbar_desc = f"Solving (Mode: {args.decode_type}, Samples: {test_samples})"
        pbar = tqdm(total=1, desc=pbar_desc, dynamic_ncols=True)
        
        # 3. ëª¨ë¸ ì¶”ë¡ 
        out = self.model(
            td, self.env, decode_type=args.decode_type, pbar=pbar, 
            log_fn=self.log, log_idx=args.log_idx,
            log_mode=args.log_mode
        )
        pbar.close()

        # 4. ìµœê³  ì„±ëŠ¥ ì†”ë£¨ì…˜ ì„ íƒ
        reward = out['reward'] # (B_total,)
        
        best_idx = reward.argmax()
        final_cost = -reward[best_idx].item()
        
        # 5. ìµœì¢… ìƒíƒœ(TensorDict) ì¶”ì¶œ
        # (tdëŠ” env.step()ì— ì˜í•´ in-placeë¡œ ìˆ˜ì •ë˜ì—ˆìœ¼ë¯€ë¡œ,
        #  model()ì´ ë°˜í™˜ëœ í›„ì˜ tdê°€ ìµœì¢… ìƒíƒœì…ë‹ˆë‹¤)
        final_td_instance = td[best_idx].clone()

        # 6. POMO ì‹œì‘ ë…¸ë“œ ì´ë¦„ ì°¾ê¸°
        best_start_node_local_idx = best_idx % test_samples
        best_start_node_idx = start_nodes_idx[best_start_node_local_idx].item()
        best_start_node_name = self.env.generator.config.node_names[best_start_node_idx]
        
        self.log(f"ì¶”ë¡  ì™„ë£Œ (Cost: ${final_cost:.4f}, Start: '{best_start_node_name}')")

        # 7. ì‹œê°í™” ì‹¤í–‰
        self.visualize_result(
            final_td_instance, 
            final_cost, 
            best_start_node_name, 
            filename_prefix
        )
        self.log(f"{log_prefix} ì‹œê°í™” ë‹¤ì´ì–´ê·¸ë¨ ì €ì¥ ì™„ë£Œ.")

    def visualize_result(self, 
                         final_td: TensorDict, 
                         final_cost: float, 
                         best_start_node_name: str, 
                         filename_prefix: str = "solution"):
        """
        Lazy Spawnì— ë§ê²Œ ìˆ˜ì •ëœ ì‹œê°í™” í•¨ìˆ˜.
        
        ìµœì¢… TensorDict ìƒíƒœ(final_td)ì˜ 'is_active_mask'ì™€ 'adj_matrix'ë¥¼
        ì½ì–´ í™œì„±í™”ëœ ë…¸ë“œì™€ ì—£ì§€ë§Œ ê·¸ë¦½ë‹ˆë‹¤.
        """
        if self.result_dir is None: return
        os.makedirs(self.result_dir, exist_ok=True)

        # 1. ì •ë³´ ì¶”ì¶œ
        node_names = self.env.generator.config.node_names # (N_max,)
        all_nodes_features = final_td["nodes"] # (N_max, D)
        adj_matrix = final_td["adj_matrix"] # (N_max, N_max)
        is_active = final_td["is_active_mask"] # (N_max,)
        
        battery_conf = self.env.generator.config.battery
        constraints = self.env.generator.config.constraints

        # 2. Graphviz ê°ì²´ ìƒì„±
        dot = Digraph(comment=f"Power Tree - Cost ${final_cost:.4f}")
        dot.attr('node', shape='box', style='rounded,filled', fontname='Arial')
        dot.attr(rankdir='LR', label=f"Transformer Solution (Start: {best_start_node_name})\nCost: ${final_cost:.4f}", labelloc='t')

        # 3. í™œì„±í™”ëœ(Active) ë…¸ë“œë§Œ ìˆœíšŒ
        active_indices = torch.where(is_active)[0]
        
        # (ì•”ì „ë¥˜/ë…ë¦½ë ˆì¼ ê³„ì‚°ì„ ìœ„í•œ ì‚¬ì „ ì‘ì—…)
        active_adj_matrix = adj_matrix[is_active][:, is_active]
        active_nodes_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(active_indices)}
        
        # 4. ë…¸ë“œ ì¶”ê°€ (Dot)
        for node_idx_tensor in active_indices:
            node_idx = node_idx_tensor.item()
            node_feat = all_nodes_features[node_idx]
            node_type = self.env.node_type_tensor[node_idx]
            node_name = node_names[node_idx] if node_idx < len(node_names) else f"Spawned_IC_{node_idx}"
            
            label = ""
            
            if node_type == NODE_TYPE_BATTERY:
                # (ì•”ì „ë¥˜ ê³„ì‚°ì€ ë³µì¡í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëµ, ì´ ì „ë ¥ë§Œ í‘œê¸°)
                label = f"ğŸ”‹ {node_name}\n\nCost: ${final_cost:.2f}"
                dot.node(node_name, label, shape='Mdiamond', color='darkgreen', fillcolor='white')
            
            elif node_type == NODE_TYPE_LOAD:
                current_active_ma = node_feat[FEATURE_INDEX["current_active"]].item() * 1000
                current_sleep_ua = node_feat[FEATURE_INDEX["current_sleep"]].item() * 1000000
                label = f"ğŸ’¡ {node_name}\nActive: {current_active_ma:.1f}mA"
                if current_sleep_ua > 0:
                    label += f"\nSleep: {current_sleep_ua:,.1f}ÂµA"
                dot.node(node_name, label, color='dimgray', fillcolor='white')

            elif node_type == NODE_TYPE_IC:
                # (ìŠ¤í°ëœ IC)
                i_out_ma = node_feat[FEATURE_INDEX["current_out"]].item() * 1000
                tj = node_feat[FEATURE_INDEX["junction_temp"]].item()
                tj_max = node_feat[FEATURE_INDEX["t_junction_max"]].item()
                cost = node_feat[FEATURE_INDEX["cost"]].item()
                
                thermal_margin = tj_max - tj
                node_color = 'blue'
                if thermal_margin < 10: node_color = 'red'
                elif thermal_margin < 25: node_color = 'orange'
                
                label = (f"ğŸ“¦ {node_name.split('@')[0]}\n\n"
                         f"Iout: {i_out_ma:.1f}mA (Active)\n"
                         f"Tj: {tj:.1f}Â°C (Max: {tj_max}Â°C)\n"
                         f"Cost: ${cost:.2f}")
                dot.node(node_name, label, color=node_color, fillcolor='lightgray', style='rounded,filled,dashed', penwidth='3')

        # 5. ì—£ì§€ ì¶”ê°€ (Dot)
        parent_indices, child_indices = adj_matrix.nonzero(as_tuple=True)
        for p_idx, c_idx in zip(parent_indices, child_indices):
            # ë‘ ë…¸ë“œ ëª¨ë‘ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì—£ì§€ë¥¼ ê·¸ë¦¼
            if is_active[p_idx] and is_active[c_idx]:
                p_name = node_names[p_idx] if p_idx < len(node_names) else f"Spawned_IC_{p_idx}"
                c_name = node_names[c_idx] if c_idx < len(node_names) else f"Spawned_IC_{c_idx}"
                dot.edge(p_name, c_name)
        
        # 6. íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_prefix}_cost_{final_cost:.4f}_{timestamp}"
        output_path = os.path.join(self.result_dir, filename)
        
        try:
            dot.render(output_path, view=False, format='png', cleanup=True)
            self.log(f"âœ… ìƒì„¸ ì‹œê°í™” ë‹¤ì´ì–´ê·¸ë¨ì„ {output_path}.png íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.log(f"âŒ ì‹œê°í™” ë Œë”ë§ ì‹¤íŒ¨. (Graphviz ì„¤ì¹˜ í™•ì¸ í•„ìš”): {e}")