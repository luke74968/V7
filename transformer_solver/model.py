# Copyright (c) 2025 Minuk Lee. All rights reserved.
# 
# This source code is proprietary and confidential.
# Unauthorized copying of this file, via any medium is strictly prohibited.
# 
# For licensing terms, see the LICENSE file.
# Contact: minuklee@snu.ac.kr
# 
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
from tensordict import TensorDict
from dataclasses import dataclass
from typing import Dict, List, Tuple

# --- í˜„ì¬ íŒ¨í‚¤ì§€(transformer_solver) ëª¨ë“ˆ ì„í¬íŠ¸ ---
from .definitions import (
    FEATURE_DIM, FEATURE_INDEX, SCALAR_PROMPT_FEATURE_DIM,
    NODE_TYPE_PADDING, NODE_TYPE_BATTERY, NODE_TYPE_LOAD, 
    NODE_TYPE_IC, NODE_TYPE_EMPTY
)
from .utils.common import batchify
from .solver_env import PocatEnv, BATTERY_NODE_IDX 


# ---
# ì„¹ì…˜ 1: í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ ë¹Œë”© ë¸”ë¡ (íš¨ìœ¨ì„±)
# ---

class RMSNorm(nn.Module):
    """ Root Mean Square Layer Normalization """
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float()).type_as(x)
        return output * self.weight

class Normalization(nn.Module):
    """ ì •ê·œí™” ë ˆì´ì–´ ë˜í¼ (RMSNorm ë˜ëŠ” LayerNorm) """
    def __init__(self, embedding_dim, norm_type='rms', **kwargs):
        super().__init__()
        self.norm_type = norm_type
        if self.norm_type == 'rms':
            self.norm = RMSNorm(embedding_dim)
        elif self.norm_type == 'layer':
            self.norm = nn.LayerNorm(embedding_dim)
        else:
            raise NotImplementedError(f"Unknown norm_type: {norm_type}")

    def forward(self, x):
        return self.norm(x)

class ParallelGatedMLP(nn.Module):
    """ SwiGLU FFN (FeedForward) êµ¬í˜„ì²´ """
    def __init__(self, hidden_size: int, **kwargs):
        super().__init__()
        # LLAMA ì•„í‚¤í…ì²˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” FFN ì°¨ì› ê³„ì‚°
        inner_size = int(2 * hidden_size * 4 / 3)
        multiple_of = 256
        inner_size = multiple_of * ((inner_size + multiple_of - 1) // multiple_of)
        
        self.l1 = nn.Linear(hidden_size, inner_size, bias=False)
        self.l2 = nn.Linear(hidden_size, inner_size, bias=False)
        self.l3 = nn.Linear(inner_size, hidden_size, bias=False)
        self.act = F.silu

    def forward(self, z):
        z1 = self.l1(z)
        z2 = self.l2(z)
        return self.l3(self.act(z1) * z2)

def reshape_by_heads(qkv: torch.Tensor, head_num: int) -> torch.Tensor:
    """ (B, N, H*D) -> (B, H, N, D) """
    batch_s, n = qkv.size(0), qkv.size(1)
    q_reshaped = qkv.reshape(batch_s, n, head_num, -1)
    return q_reshaped.transpose(1, 2)

def multi_head_attention(q, k, v, attention_mask=None):
    """ 
    í‘œì¤€ Multi-Head Attention êµ¬í˜„.
    (attention_maskê°€ bool íƒ€ì…ì˜ (B, ..., N, N)ì´ë¼ê³  ê°€ì •)
    """
    batch_s, head_num, n, key_dim = q.shape
    
    # 1. ìŠ¤ì½”ì–´ ê³„ì‚°
    score = torch.matmul(q, k.transpose(2, 3))
    score_scaled = score / (key_dim ** 0.5)
    
    # 2. ì–´í…ì…˜ ë§ˆìŠ¤í‚¹ (ë§ˆìŠ¤í¬ê°€ 0/Falseì¸ ìœ„ì¹˜ë¥¼ -infë¡œ)
    if attention_mask is not None:
        if attention_mask.dim() == 3:
            attention_mask = attention_mask.unsqueeze(1) # (B, N, N) -> (B, 1, N, N)
        
        score_scaled = score_scaled.masked_fill(attention_mask == 0, -1e12)
        
    # 3. Softmax ë° Value ì ìš©
    weights = nn.Softmax(dim=3)(score_scaled)
    out = torch.matmul(weights, v)
    
    # 4. (B, H, N, D) -> (B, N, H*D)
    out_transposed = out.transpose(1, 2)
    return out_transposed.contiguous().view(batch_s, n, head_num * key_dim)

class EncoderLayer(nn.Module):
    """ 
    í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë ˆì´ì–´ (Post-Normalization)
    """
    def __init__(self, embedding_dim, head_num, qkv_dim, ffd='siglu', **model_params):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.head_num = head_num
        self.qkv_dim = qkv_dim
        
        self.Wq = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)
        
        self.normalization1 = Normalization(embedding_dim, **model_params)
        
        if ffd == 'siglu':
            self.feed_forward = ParallelGatedMLP(hidden_size=embedding_dim, **model_params)
        else:
            raise NotImplementedError
            
        self.normalization2 = Normalization(embedding_dim, **model_params)

    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:
        # 1. MHA (Post-Normalization)
        q = reshape_by_heads(self.Wq(x), self.head_num)
        k = reshape_by_heads(self.Wk(x), self.head_num)
        v = reshape_by_heads(self.Wv(x), self.head_num)
        
        mha_out = self.multi_head_combine(multi_head_attention(q, k, v, attention_mask=attention_mask))
        h = self.normalization1(x + mha_out) # Residual + Norm
        
        # 2. FFN (Post-Normalization)
        ffn_out = self.feed_forward(h)
        out = self.normalization2(h + ffn_out) # Residual + Norm
        return out

class PocatDecoderLayer(nn.Module):
    """
    Cross-Attentionê³¼ FFNìœ¼ë¡œ êµ¬ì„±ëœ ë””ì½”ë” ë ˆì´ì–´
    (Queryê°€ 1ê°œì´ë¯€ë¡œ Self-Attentionì€ ìƒëµí•˜ê³  Cross-Attentionì— ì§‘ì¤‘)
    """
    def __init__(self, embedding_dim, head_num, qkv_dim, **model_params):
        super().__init__()
        
        # 1. Cross-Attention (QueryëŠ” ì´ì „ ë ˆì´ì–´ ì¶œë ¥, Key/Valì€ ì¸ì½”ë” ì¶œë ¥)
        self.Wq = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        # (Wk, WvëŠ” ì¸ì½”ë” ìª½ì—ì„œ ë¯¸ë¦¬ ê³„ì‚°ëœ ìºì‹œë¥¼ ì¬ì‚¬ìš©í•˜ê±°ë‚˜, ì—¬ê¸°ì„œ ë³„ë„ ì •ì˜ ê°€ëŠ¥)
        # íš¨ìœ¨ì„±ì„ ìœ„í•´ ì—¬ê¸°ì„œëŠ” ì¸ì½”ë”ì˜ K, Vë¥¼ ê³µìœ (Sharing)í•˜ê±°ë‚˜ 
        # ë³„ë„ë¡œ íˆ¬ì˜(Projection)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë³„ë„ íˆ¬ì˜ì„ ê°€ì •í•©ë‹ˆë‹¤.
        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)
        
        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)
        
        self.norm1 = Normalization(embedding_dim, **model_params)
        self.norm2 = Normalization(embedding_dim, **model_params)
        
        # 2. Feed Forward Network
        self.feed_forward = ParallelGatedMLP(hidden_size=embedding_dim, **model_params)
        
        self.head_num = head_num
        self.qkv_dim = qkv_dim

    def forward(self, x, encoder_out):
        """
        x: (B, 1, D) - í˜„ì¬ ë””ì½”ë”ì˜ Query ìƒíƒœ
        encoder_out: (B, N, D) - ì¸ì½”ë” ì¶œë ¥ (Context)
        """
        # --- Cross Attention ---
        # Query: í˜„ì¬ ë ˆì´ì–´ì˜ ì…ë ¥ x
        q = reshape_by_heads(self.Wq(x), self.head_num)
        
        # Key, Value: ì¸ì½”ë” ì¶œë ¥ (ë§¤ ë ˆì´ì–´ë§ˆë‹¤ ìƒˆë¡œ ê³„ì‚°í•˜ì—¬ í‘œí˜„ë ¥ ì¦ëŒ€)
        k = reshape_by_heads(self.Wk(encoder_out), self.head_num)
        v = reshape_by_heads(self.Wv(encoder_out), self.head_num)
        
        mha_out = multi_head_attention(q, k, v)
        mha_out = self.multi_head_combine(mha_out)
        
        h = self.norm1(x + mha_out) # Residual + Norm
        
        # --- FFN ---
        ffn_out = self.feed_forward(h)
        out = self.norm2(h + ffn_out) # Residual + Norm
        
        return out
# ---
# ì„¹ì…˜ 2: ë””ì½”ë”© íš¨ìœ¨ì„ ìœ„í•œ ìºì‹œ
# ---

@dataclass
class PrecomputedCache:
    """
    ë””ì½”ë”© ë£¨í”„ì—ì„œ ë°˜ë³µ ê³„ì‚°ì„ í”¼í•˜ê¸° ìœ„í•´
    ì¸ì½”ë”ì˜ Key, Value ê°’ì„ ì €ì¥í•˜ëŠ” ìºì‹œ ê°ì²´ì…ë‹ˆë‹¤.
    """
    node_embeddings: torch.Tensor
    #glimpse_key: torch.Tensor
    #glimpse_val: torch.Tensor
    logit_key_connect: torch.Tensor # 'Connect' í¬ì¸í„°ìš© Key
    logit_key_spawn: torch.Tensor   # 'Spawn' í¬ì¸í„°ìš© Key

    def batchify(self, num_starts: int):
        """ POMO ìƒ˜í”Œë§ì„ ìœ„í•´ ìºì‹œë¥¼ N_starts ë°°ìˆ˜ë§Œí¼ ë³µì œí•©ë‹ˆë‹¤. """
        return PrecomputedCache(
            batchify(self.node_embeddings, num_starts),
            #batchify(self.glimpse_key, num_starts),
            #batchify(self.glimpse_val, num_starts),
            batchify(self.logit_key_connect, num_starts),
            batchify(self.logit_key_spawn, num_starts),
        )

# ---
# ì„¹ì…˜ 3: POCAT ëª¨ë¸ ì•„í‚¤í…ì²˜
# ---

class PocatPromptNet(nn.Module):
    """
    ìŠ¤ì¹¼ë¼/í–‰ë ¬ ì œì•½ì¡°ê±´ì„ ì„ë² ë”©í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ë„¤íŠ¸ì›Œí¬ (N_MAX ëŒ€ì‘)
    """
    def __init__(self, embedding_dim: int, N_MAX: int, **kwargs):
        super().__init__()
        self.scalar_net = nn.Sequential(
            nn.Linear(SCALAR_PROMPT_FEATURE_DIM, embedding_dim // 2),
            nn.ReLU(),
            nn.Linear(embedding_dim // 2, embedding_dim // 2)
        )
        self.matrix_net = nn.Sequential(
            nn.Linear(N_MAX * N_MAX, embedding_dim),
            nn.ReLU(),
            nn.Linear(embedding_dim, embedding_dim // 2)
        )
        self.final_processor = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim),
            nn.LayerNorm(embedding_dim),
            nn.ReLU()
        )

    def forward(self, scalar_features: torch.Tensor, matrix_features: torch.Tensor) -> torch.Tensor:
        scalar_embedding = self.scalar_net(scalar_features)
        batch_size = matrix_features.shape[0]
        matrix_flat = matrix_features.view(batch_size, -1) # (B, N_MAX*N_MAX)
        matrix_embedding = self.matrix_net(matrix_flat)
        combined_embedding = torch.cat([scalar_embedding, matrix_embedding], dim=-1)
        final_prompt_embedding = self.final_processor(combined_embedding)
        return final_prompt_embedding.unsqueeze(1) # (B, 1, D)


class PocatEncoder(nn.Module):
    """
    Pocat ì¸ì½”ë” (ë“€ì–¼ ì–´í…ì…˜ ë° ë‹¤ì¤‘ ì„ë² ë”© ì£¼ì…).
    
    1. ë…¸ë“œ íƒ€ì…(5ì¢…)ë³„ë¡œ ê¸°ë³¸ ì„ë² ë”© ì ìš©
    2. ë…¸ë“œ ì†ì„±/ìƒíƒœ(4ì¢…)ë³„ë¡œ ì¶”ê°€ ì„ë² ë”© ì£¼ì…
    3. ë“€ì–¼ ì–´í…ì…˜(Sparse/Global) í†µê³¼
    """
    def __init__(self, embedding_dim: int, encoder_layer_num: int, **model_params):
        super().__init__()
        
        # 1. ë…¸ë“œ "íƒ€ì…" (5ì¢…) ì„ë² ë”©
        self.embedding_padding = nn.Linear(FEATURE_DIM, embedding_dim)
        self.embedding_battery = nn.Linear(FEATURE_DIM, embedding_dim)
        self.embedding_load = nn.Linear(FEATURE_DIM, embedding_dim)
        self.embedding_ic = nn.Linear(FEATURE_DIM, embedding_dim)
        self.embedding_empty = nn.Linear(FEATURE_DIM, embedding_dim)
        
        # 2. ë…¸ë“œ "ì†ì„±/ìƒíƒœ" (4ì¢…) ì„ë² ë”© (0 ë˜ëŠ” 1 ê°’ì„ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©)
        self.embedding_is_active = nn.Embedding(2, embedding_dim)
        self.embedding_is_template = nn.Embedding(2, embedding_dim)
        self.embedding_can_spawn_into = nn.Embedding(2, embedding_dim)
        self.embedding_rail_type = nn.Embedding(3, embedding_dim) # 0:N/A, 1:Supp, 2:Path

        # 3. ë“€ì–¼ ì–´í…ì…˜(CaDA) ë ˆì´ì–´
        self.sparse_layers = nn.ModuleList([
            EncoderLayer(embedding_dim=embedding_dim, **model_params) 
            for _ in range(encoder_layer_num)
        ])
        self.global_layers = nn.ModuleList([
            EncoderLayer(embedding_dim=embedding_dim, **model_params) 
            for _ in range(encoder_layer_num)
        ])
        self.sparse_fusion = nn.ModuleList([
            nn.Linear(embedding_dim, embedding_dim) 
            for _ in range(encoder_layer_num)
        ])
        self.global_fusion = nn.ModuleList([
            nn.Linear(embedding_dim, embedding_dim) 
            for _ in range(encoder_layer_num - 1)
        ])

    def forward(self, td: TensorDict, prompt_embedding: torch.Tensor) -> torch.Tensor:
        node_features = td['nodes'] # (B, N_MAX, 27)
        batch_size, num_nodes, _ = node_features.shape # num_nodes = N_MAX
        embedding_dim = self.embedding_battery.out_features
        
        node_embeddings = torch.zeros(batch_size, num_nodes, embedding_dim, device=node_features.device)
        
        # --- 1. íƒ€ì…ë³„ ê¸°ë³¸ ì„ë² ë”© ì ìš© ---
        node_type_indices = node_features[..., FEATURE_INDEX["node_type"][0]:FEATURE_INDEX["node_type"][1]].argmax(dim=-1)
        
        masks = {
            NODE_TYPE_PADDING: (node_type_indices == NODE_TYPE_PADDING),
            NODE_TYPE_BATTERY: (node_type_indices == NODE_TYPE_BATTERY),
            NODE_TYPE_LOAD: (node_type_indices == NODE_TYPE_LOAD),
            NODE_TYPE_IC: (node_type_indices == NODE_TYPE_IC),
            NODE_TYPE_EMPTY: (node_type_indices == NODE_TYPE_EMPTY),
        }
        
        if masks[NODE_TYPE_PADDING].any(): node_embeddings[masks[NODE_TYPE_PADDING]] = self.embedding_padding(node_features[masks[NODE_TYPE_PADDING]])
        if masks[NODE_TYPE_BATTERY].any(): node_embeddings[masks[NODE_TYPE_BATTERY]] = self.embedding_battery(node_features[masks[NODE_TYPE_BATTERY]])
        if masks[NODE_TYPE_LOAD].any(): node_embeddings[masks[NODE_TYPE_LOAD]] = self.embedding_load(node_features[masks[NODE_TYPE_LOAD]])
        if masks[NODE_TYPE_IC].any(): node_embeddings[masks[NODE_TYPE_IC]] = self.embedding_ic(node_features[masks[NODE_TYPE_IC]])
        if masks[NODE_TYPE_EMPTY].any(): node_embeddings[masks[NODE_TYPE_EMPTY]] = self.embedding_empty(node_features[masks[NODE_TYPE_EMPTY]])

        # --- 2. ì†ì„±/ìƒíƒœ ì„ë² ë”© ì£¼ì… (Injection) ---
        active_ids = node_features[..., FEATURE_INDEX["is_active"]].long()
        template_ids = node_features[..., FEATURE_INDEX["is_template"]].long()
        spawn_ids = node_features[..., FEATURE_INDEX["can_spawn_into"]].long()
        rail_ids = node_features[..., FEATURE_INDEX["independent_rail_type"]].round().long().clamp(0, 2)
        
        node_embeddings.add_(self.embedding_is_active(active_ids))
        node_embeddings.add_(self.embedding_is_template(template_ids))
        node_embeddings.add_(self.embedding_can_spawn_into(spawn_ids))
        node_embeddings.add_(self.embedding_rail_type(rail_ids))
        
        # --- 3. ë“€ì–¼ ì–´í…ì…˜ (CaDA) ì‹¤í–‰ ---
        connectivity_mask = td['connectivity_matrix'] # (B, N_MAX, N_MAX)
        attention_mask = td['attention_mask'] # (B, N_MAX, N_MAX)

        global_input = torch.cat((node_embeddings, prompt_embedding), dim=1)
        
        global_attention_mask = torch.zeros(
            batch_size, num_nodes + 1, num_nodes + 1, 
            dtype=torch.bool, device=node_embeddings.device
        )
        global_attention_mask[:, :num_nodes, :num_nodes] = attention_mask
        
        alive_mask_1d = (node_type_indices != NODE_TYPE_PADDING)
        global_attention_mask[:, num_nodes, :num_nodes] = alive_mask_1d
        global_attention_mask[:, :num_nodes, num_nodes] = alive_mask_1d
        global_attention_mask[:, num_nodes, num_nodes] = True
        
        sparse_out, global_out = node_embeddings, global_input
        for i in range(len(self.sparse_layers)):
            sparse_out = self.sparse_layers[i](sparse_out, attention_mask=connectivity_mask)
            global_out = self.global_layers[i](global_out, attention_mask=global_attention_mask)
            
            sparse_out = sparse_out + self.sparse_fusion[i](global_out[:, :num_nodes])
            if i < len(self.global_layers) - 1:
                global_nodes = global_out[:, :num_nodes] + self.global_fusion[i](sparse_out)
                global_out = torch.cat((global_nodes, global_out[:, num_nodes:]), dim=1)  
                
        return global_out[:, :num_nodes] # í”„ë¡¬í”„íŠ¸ ì„ë² ë”© ì œì™¸ (B, N_MAX, D)


class PocatDecoder(nn.Module):
    def __init__(self, embedding_dim, head_num, qkv_dim, N_MAX, **model_params):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.head_num = head_num
        self.qkv_dim = qkv_dim
        self.N_MAX = N_MAX
        
        # config.yamlì—ì„œ decoder_layer_numì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (ê¸°ë³¸ê°’ 1)
        self.layer_num = model_params.get('decoder_layer_num', 1)

        # 1. ì´ˆê¸° ì»¨í…ìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±ìš© (ì…ë ¥ ì°¨ì› ë³€í™˜)
        # (embedding_dim + 3 features) -> embedding_dim
        self.input_projector = nn.Linear(embedding_dim + 3, embedding_dim)

        # 2. ë””ì½”ë” ë ˆì´ì–´ ìŠ¤íƒ (ModuleList)
        self.layers = nn.ModuleList([
            PocatDecoderLayer(embedding_dim, head_num, qkv_dim, **model_params)
            for _ in range(self.layer_num)
        ])
        
        # 3. í¬ì¸í„° ë„¤íŠ¸ì›Œí¬ìš© Key ìƒì„± (ì¸ì½”ë” ì„ë² ë”©ì„ ë³€í™˜)
        self.Wk_connect_logit = nn.Linear(embedding_dim, embedding_dim, bias=False)
        self.Wk_spawn_logit = nn.Linear(embedding_dim, embedding_dim, bias=False)

        # --- 4. 4-Heads (q_vecì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ) ---
        self.value_head = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim // 2),
            nn.ReLU(),
            nn.Linear(embedding_dim // 2, 1)
        )
        self.type_head = nn.Linear(embedding_dim, 2)
        self.connect_head = nn.Linear(embedding_dim, embedding_dim)
        self.spawn_head = nn.Linear(embedding_dim, embedding_dim)

    def forward(self, td: TensorDict, cache: PrecomputedCache) -> Tuple[torch.Tensor, ...]:
        
        # 1. ì´ˆê¸° ì¿¼ë¦¬ ì…ë ¥ ì¤€ë¹„
        avg_current = td["nodes"][..., FEATURE_INDEX["current_out"]].clone().mean(dim=1, keepdim=True)
        unconnected_ratio = td["unconnected_loads_mask"].clone().float().mean(dim=1, keepdim=True)
        step_ratio = td["step_count"].clone().float() / (2 * self.N_MAX)
        state_features = torch.cat([avg_current, unconnected_ratio, step_ratio], dim=1)

        head_idx = td["trajectory_head"].detach().squeeze(-1).clone()
        batch_indices = torch.arange(td.batch_size[0], device=head_idx.device)
        head_emb = cache.node_embeddings[batch_indices, head_idx]
        
        # (B, D+3) -> (B, 1, D)
        query_input = torch.cat([head_emb, state_features], dim=1).unsqueeze(1)
        
        # ì´ˆê¸° q_vec (Projection)
        q_vec = self.input_projector(query_input)

        # 2. ë””ì½”ë” ë ˆì´ì–´ ìˆœì°¨ í†µê³¼ (Stacking)
        # q_vecì´ ê° ë ˆì´ì–´ë¥¼ ê±°ì¹˜ë©° ì ì  ë” ì •êµí•œ Context Vectorê°€ ë©ë‹ˆë‹¤.
        encoder_out = cache.node_embeddings # (B, N, D)
        
        for layer in self.layers:
            q_vec = layer(q_vec, encoder_out)

        # --- 3. ìµœì¢… ê²°ì • (Heads) ---
        value = self.value_head(q_vec).squeeze(-1)
        logits_action_type = self.type_head(q_vec).squeeze(1)
        
        query_connect = self.connect_head(q_vec) 
        logits_connect_target = torch.matmul(
            query_connect, cache.logit_key_connect
        ).squeeze(1) / (self.embedding_dim ** 0.5)
        
        query_spawn = self.spawn_head(q_vec) 
        logits_spawn_template = torch.matmul(
            query_spawn, cache.logit_key_spawn
        ).squeeze(1) / (self.embedding_dim ** 0.5)

        return logits_action_type, logits_connect_target, logits_spawn_template, value

class PocatModel(nn.Module):
    """
    Pocat V7 (Padding + Lazy Spawn) ë©”ì¸ ëª¨ë¸
    """
    
    def __init__(self, **model_params):
        super().__init__()
        self.logit_clipping = model_params.get('logit_clipping', 10)
        
        # config.yamlì—ì„œ N_MAX ì£¼ì…
        self.N_MAX = model_params['N_MAX']
        # model_paramsì—ì„œ N_MAXë¥¼ popí•˜ì—¬ ì¤‘ë³µ ì „ë‹¬ ë°©ì§€
        # (PocatPromptNetê³¼ PocatDecoderëŠ” N_MAXë¥¼ ëª…ì‹œì  ì¸ìë¡œ ë°›ìŒ)s
        n_max_value = model_params.pop('N_MAX')
        self.prompt_net = PocatPromptNet(N_MAX=n_max_value, **model_params)
        self.encoder = PocatEncoder(**model_params)
        self.decoder = PocatDecoder(N_MAX=n_max_value, **model_params)

    def _get_masked_probs(self, logits, mask):
        """ ë¡œì§“ê³¼ ë§ˆìŠ¤í¬ë¥¼ ë°›ì•„ ì •ê·œí™”ëœ í™•ë¥  ë¶„í¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. """
        scores = self.logit_clipping * torch.tanh(logits)
        scores.masked_fill_(~mask, -float('inf'))
        probs = F.softmax(scores, dim=-1)
        return probs  

    def _sample_action(self, logits, mask, decode_type):
        """ 
        ë¡œì§“ê³¼ ë§ˆìŠ¤í¬ë¥¼ ë°›ì•„ ì•¡ì…˜(idx)ê³¼ ë¡œê·¸ í™•ë¥ (log_prob)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        (ë§‰ë‹¤ë¥¸ ê¸¸ ë°©ì§€ ë¡œì§ í¬í•¨)
        """
        scores = self.logit_clipping * torch.tanh(logits)
        scores.masked_fill_(~mask, -float('inf'))
        
        # ëª¨ë“  ì•¡ì…˜ì´ ë§ˆìŠ¤í‚¹ëœ 'ë§‰ë‹¤ë¥¸ ê¸¸' ìƒíƒœ ë°©ì§€
        is_stuck = torch.all(scores == -float('inf'), dim=-1)
        scores[is_stuck, 0] = 0.0 # (0ë²ˆ ì¸ë±ìŠ¤(ë°°í„°ë¦¬)ë¼ë„ ê°•ì œ ì„ íƒ)
        
        log_prob = F.log_softmax(scores, dim=-1)
        probs = log_prob.exp()
        
        if decode_type == 'greedy':
            action = probs.argmax(dim=-1)
        else: # 'sampling'
            action = Categorical(probs=probs).sample()
            
        # ì„ íƒëœ ì•¡ì…˜ì˜ ë¡œê·¸ í™•ë¥  ë°˜í™˜
        return action, log_prob.gather(1, action.unsqueeze(-1)).squeeze(-1)

    def _combine_log_probs(self, 
                           log_prob_type, action_type, 
                           log_prob_connect, log_prob_spawn):
        """
        Parameterized Actionì˜ ë¡œê·¸ í™•ë¥ ì„ ê²°í•©í•©ë‹ˆë‹¤.
        logÏ€(a|s) = logÏ€(type|s) + logÏ€(arg|type,s)
        """
        # 'Connect' (0)ë¥¼ ì„ íƒí•œ ê²½ìš°ì˜ ë¡œê·¸ í™•ë¥ 
        log_prob_if_connect = log_prob_type + log_prob_connect
        # 'Spawn' (1)ì„ ì„ íƒí•œ ê²½ìš°ì˜ ë¡œê·¸ í™•ë¥ 
        log_prob_if_spawn = log_prob_type + log_prob_spawn
        
        # (B,)
        final_log_prob = torch.where(
            action_type == 0,       # 'Connect'ë¥¼ ì„ íƒí–ˆìœ¼ë©´
            log_prob_if_connect,    # ì´ í™•ë¥ ì„ ì‚¬ìš©
            log_prob_if_spawn       # ì•„ë‹ˆë©´ (Spawn) ì´ í™•ë¥ ì„ ì‚¬ìš©
        )
        return final_log_prob

    def forward(self, 
                td: TensorDict, 
                env: PocatEnv, # (solver_env.pyì˜ í™˜ê²½ ê°ì²´)
                decode_type: str = 'greedy', 
                pbar: object = None,
                status_msg: str = "", 
                log_fn=None, log_idx: int = 0, 
                log_mode: str = 'progress',
                return_final_td: bool = False,   # ğŸ‘ˆ ì´ ì¤„ ì¶”ê°€
                ) -> Dict[str, torch.Tensor]:
        
        base_desc = pbar.desc.split(' | ')[0] if pbar else ""
        if pbar: pbar.set_description(f"{base_desc} | {status_msg} | â–¶ Encoding")
        
        # --- 1. ì¸ì½”ë”© ë° ìºì‹œ ìƒì„± ---
        prompt_embedding = self.prompt_net(td["scalar_prompt_features"], td["matrix_prompt_features"])
        encoded_nodes = self.encoder(td, prompt_embedding) # (B, N_MAX, D)
        
        # ë””ì½”ë”ê°€ ì‚¬ìš©í•  Key/Value ì‚¬ì „ ê³„ì‚°
        #glimpse_key = reshape_by_heads(self.decoder.Wk_glimpse(encoded_nodes), self.decoder.head_num)
        #glimpse_val = reshape_by_heads(self.decoder.Wv_glimpse(encoded_nodes), self.decoder.head_num)
        
        # í¬ì¸í„° í—¤ë“œë³„ Key ìƒì„±
        logit_key_connect = self.decoder.Wk_connect_logit(encoded_nodes).transpose(1, 2)
        logit_key_spawn = self.decoder.Wk_spawn_logit(encoded_nodes).transpose(1, 2)
        
        cache = PrecomputedCache(
            node_embeddings=encoded_nodes,
            #glimpse_key=glimpse_key,
            #glimpse_val=glimpse_val,
            logit_key_connect=logit_key_connect,
            logit_key_spawn=logit_key_spawn
        )
        
        # --- 2. POMO (Multi-Start) ì¤€ë¹„ ---
        num_starts, start_nodes_idx = env.select_start_nodes(td)
        if num_starts == 0:
             # (B, 1) í˜•íƒœì˜ 0ì  ë¦¬ì›Œë“œ ë°˜í™˜
            zero_reward = torch.zeros(td.batch_size[0], 1, device=td.device)
            return {"reward": zero_reward} # (POMO ì‹œì‘ ë¶ˆê°€)

        num_total_loads = env.generator.num_loads
        batch_size = td.batch_size[0]
        
        # (B) -> (B * num_starts)
        td_expanded_view = batchify(td, num_starts)
        td = td_expanded_view
        cache = cache.batchify(num_starts) # ìºì‹œë„ í™•ì¥

        # POMO ì‹œì‘: ì²« ì•¡ì…˜(Load ì„ íƒ)ì„ í™˜ê²½ì— ê°•ì œ ì ìš©
        first_action_tensor = start_nodes_idx.repeat(batch_size).unsqueeze(-1)
        
        # (POMOì˜ ì²« ìŠ¤í…ì€ env._resetì—ì„œ ì²˜ë¦¬ë˜ë„ë¡ solver_env.pyì—ì„œ êµ¬í˜„ í•„ìš”)
        # (ì—¬ê¸°ì„œëŠ” tdê°€ ì´ë¯¸ ì²« Loadê°€ Headë¡œ ì„¤ì •ëœ ìƒíƒœë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.)
        
        # --- 3. ë””ì½”ë”© ë£¨í”„ ---
        log_probs: List[torch.Tensor] = []
        actions: List[Dict[str, torch.Tensor]] = []
        rewards: List[torch.Tensor] = []
        first_value: torch.Tensor = None
        
        decoding_step = 0
        while not td["done"].all():
            decoding_step += 1
            if pbar and log_mode == 'progress':
                # (ì§„í–‰ë¥  í‘œì‹œ: 0ë²ˆ ìƒ˜í”Œ ê¸°ì¤€)
                unconnected = td['unconnected_loads_mask'][0].sum().item()
                connected = num_total_loads - unconnected
                pbar.set_description(f"{base_desc} | {status_msg} | Loads {connected}/{num_total_loads}")

            # 1. ë””ì½”ë” í˜¸ì¶œ (4ê°œ í…ì„œ ë°˜í™˜)
            logits_type, logits_connect, logits_spawn, value = self.decoder(td, cache)
            
            # A2Cë¥¼ ìœ„í•´ ì²« ìŠ¤í…ì˜ Value(ê°€ì¹˜) ì €ì¥
            if decoding_step == 1:
                first_value = value.squeeze(-1) # (B * N_loads, 1) -> (B * N_loads)
            
            # 2. í™˜ê²½ì—ì„œ 3ì¢… ë§ˆìŠ¤í¬ ê°€ì ¸ì˜¤ê¸°
            # (solver_env.pyê°€ ë°˜í™˜í•  ë§ˆìŠ¤í¬ ë”•ì…”ë„ˆë¦¬)
            with torch.no_grad():
                masks: Dict[str, torch.Tensor] = env.get_action_mask(td)
            
            # 3. 3ê°œ í—¤ë“œì—ì„œ ê°ê° ìƒ˜í”Œë§
            action_type, log_prob_type = self._sample_action(
                logits_type, masks["mask_type"], decode_type
            )
            action_connect, log_prob_connect = self._sample_action(
                logits_connect, masks["mask_connect"], decode_type
            )
            action_spawn, log_prob_spawn = self._sample_action(
                logits_spawn, masks["mask_spawn"], decode_type
            )

            # 4. Parameterized Action Log Prob ê²°í•©
            final_log_prob = self._combine_log_probs(
                log_prob_type, action_type, 
                log_prob_connect, log_prob_spawn
            )
            
            # 5. í™˜ê²½ì— ì „ë‹¬í•  ì•¡ì…˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            action_dict = {
                "action_type": action_type.unsqueeze(-1),
                "connect_target": action_connect.unsqueeze(-1),
                "spawn_template": action_spawn.unsqueeze(-1),
            }
            
            # [START]: 'detail' ëª¨ë“œ ì•¡ì…˜ ë¡œê¹… (ìˆ˜ì •ë¨)
            if log_fn and log_mode == 'detail':
                # (ì²« ë²ˆì§¸ ìƒ˜í”Œ(B=0) ê¸°ì¤€ìœ¼ë¡œ ë¡œê·¸ ì¶œë ¥)
                sample_idx = 0
                if sample_idx < td.batch_size[0]:
                    current_head = td["trajectory_head"][sample_idx].item()
                    
                    # --- 1. í™•ë¥  ë¶„í¬ ê³„ì‚° ---
                    # (ìœ„ì—ì„œ ì •ì˜í•œ _get_masked_probs ì‚¬ìš©)
                    probs_type = self._get_masked_probs(logits_type[sample_idx], masks["mask_type"][sample_idx])
                    probs_connect = self._get_masked_probs(logits_connect[sample_idx], masks["mask_connect"][sample_idx])
                    probs_spawn = self._get_masked_probs(logits_spawn[sample_idx], masks["mask_spawn"][sample_idx])

                    # --- 2. ì´ë¦„ ë§¤í•‘ ì¤€ë¹„ ---
                    # (í™˜ê²½ ì„¤ì •ì—ì„œ ì •ì  ì´ë¦„ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°)
                    node_names = env.generator.config.node_names
                    def get_name(idx):
                        if 0 <= idx < len(node_names): return node_names[idx]
                        return f"Spawned_Node_{idx}" # ë™ì  ìƒì„±ëœ ë…¸ë“œëŠ” ì¸ë±ìŠ¤ë¡œ í‘œì‹œ

                    head_name = get_name(current_head)
                    
                    log_fn(f"\n[Step {decoding_step:02d}] Current Head: {head_name} (idx: {current_head})")

                    # --- 3. Action Type í™•ë¥  ì¶œë ¥ ---
                    p_conn = probs_type[0].item()
                    p_spwn = probs_type[1].item()
                    
                    chosen_type = action_type[sample_idx].item()
                    type_str = "Connect" if chosen_type == 0 else "Spawn"
                    
                    log_fn(f"  ğŸ“Š Action Type Probabilities:")
                    log_fn(f"     - Connect: {p_conn*100:.2f}% {'ğŸ‘ˆ Selected' if chosen_type==0 else ''}")
                    log_fn(f"     - Spawn  : {p_spwn*100:.2f}% {'ğŸ‘ˆ Selected' if chosen_type==1 else ''}")

                    # --- 4. ìƒì„¸ í›„ë³´ í™•ë¥  ì¶œë ¥ ---
                    
                    # (A) Connect í›„ë³´ë“¤
                    if masks["mask_type"][sample_idx, 0]: # Connectê°€ ê°€ëŠ¥í•œ ê²½ìš°ë§Œ
                        log_fn(f"  ğŸ”— Connect Candidates (P(Target | Connect)):")
                        valid_connect_indices = torch.where(masks["mask_connect"][sample_idx])[0]
                        
                        # í™•ë¥ ìˆœ ì •ë ¬
                        cand_probs = []
                        for idx in valid_connect_indices:
                            prob = probs_connect[idx].item()
                            cand_probs.append((prob, idx.item()))
                        cand_probs.sort(key=lambda x: x[0], reverse=True)

                        for prob, idx in cand_probs:
                            name = get_name(idx)
                            is_picked = (chosen_type == 0 and action_connect[sample_idx].item() == idx)
                            log_fn(f"     - {name:<25} : {prob*100:.2f}% {'âœ…' if is_picked else ''}")
                    
                    # (B) Spawn í›„ë³´ë“¤
                    if masks["mask_type"][sample_idx, 1]: # Spawnì´ ê°€ëŠ¥í•œ ê²½ìš°ë§Œ
                        log_fn(f"  ğŸ“¦ Spawn Candidates (P(Template | Spawn)):")
                        valid_spawn_indices = torch.where(masks["mask_spawn"][sample_idx])[0]
                        
                        cand_probs = []
                        for idx in valid_spawn_indices:
                            prob = probs_spawn[idx].item()
                            cand_probs.append((prob, idx.item()))
                        cand_probs.sort(key=lambda x: x[0], reverse=True)

                        for prob, idx in cand_probs:
                            name = get_name(idx)
                            is_picked = (chosen_type == 1 and action_spawn[sample_idx].item() == idx)
                            log_fn(f"     - {name:<25} : {prob*100:.2f}% {'âœ…' if is_picked else ''}")

                    log_fn("-" * 60)
            # [END]: 'detail' ëª¨ë“œ ì•¡ì…˜ ë¡œê¹…

            # 6. í™˜ê²½ ìŠ¤í… ì‹¤í–‰
            with torch.no_grad():
                td.set("action", action_dict)
                output_td = env.step(td)
            
            reward = output_td["reward"]
            td = output_td["next"]
            
            # 7. A2C í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
            log_probs.append(final_log_prob)
            actions.append(action_dict)
            rewards.append(reward)

        # 8. ìµœì¢… ê²°ê³¼ ì·¨í•©
        if not rewards:
            # (ë””ì½”ë”© ë£¨í”„ê°€ 1ë²ˆë„ ëŒì§€ ì•Šì€ ê²½ìš° - ì˜ˆ: ì´ë¯¸ ì™„ë£Œëœ ìƒíƒœ)
            B_total = td.batch_size[0]
            dummy_reward = torch.zeros(B_total, 1, device=td.device)
            dummy_log_prob = torch.zeros(B_total, device=td.device)
            dummy_value = torch.zeros(B_total, 1, device=td.device)
            return {
                "reward": dummy_reward,
                "log_likelihood": dummy_log_prob,
                "actions": [],
                "value": dummy_value,
            }

        # (B_total, T) -> (B_total, 1)
        total_reward = torch.stack(rewards, 1).sum(1)
        # (B_total, T) -> (B_total)
        total_log_likelihood = torch.stack(log_probs, 1).sum(1)

        # [ì¶”ê°€] ìµœì¢… ìƒíƒœì—ì„œ ë¹„ìš© ì •ë³´ ì¶”ì¶œ
        final_bom_cost = td["current_cost"].squeeze(-1)
        final_sleep_cost = td["sleep_cost"].squeeze(-1)


        result = {
            "reward": total_reward,
            "log_likelihood": total_log_likelihood,
            "actions": actions,  # (ë””ë²„ê¹…ìš©)
            "value": first_value,
            "bom_cost": final_bom_cost, # [ì¶”ê°€]
            "sleep_cost": final_sleep_cost, # [ì¶”ê°€]
        }

        if return_final_td:
            # ì‹œê°í™”/ë””ë²„ê¹…ìš© ìµœì¢… ìƒíƒœëŠ” GPU ì „ì²´ TensorDictë¥¼ í†µì§¸ë¡œ
            # clone() í•˜ëŠ” ëŒ€ì‹ ,
            #  - ê·¸ë˜ë””ì–¸íŠ¸ ì—°ê²°ì„ ëŠê³ (detach)
            #  - í•„ìš”í•œ í‚¤ë§Œ ê³¨ë¼ì„œ
            #  - CPU ë©”ëª¨ë¦¬ë¡œë§Œ ì €ì¥í•œë‹¤.
            #
            # visualize_result()ì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤:
            #   - "nodes"
            #   - "adj_matrix"
            #   - "is_active_mask"
            final_td_cpu = TensorDict(
                {
                    "nodes": td["nodes"].detach().cpu(),
                    "adj_matrix": td["adj_matrix"].detach().cpu(),
                    "is_active_mask": td["is_active_mask"].detach().cpu(),
                },
                batch_size=td.batch_size,
            )
            result["final_td"] = final_td_cpu


        return result